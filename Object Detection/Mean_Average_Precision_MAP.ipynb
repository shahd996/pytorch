{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c258c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3620be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    # boxes_preds - N x 4, where N is # boxes\n",
    "    if box_format == 'midpoint':\n",
    "        # [x,y,w,h]\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2 # N x 1\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 2:3] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 3:4] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 2:3] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 3:4] + boxes_labels[..., 3:4] / 2\n",
    "        \n",
    "    if(box_format == 'corners'):\n",
    "        # [x1,y1,x2,y2]\n",
    "        box1_x1 = boxes_preds[..., 0:1] # N x 1\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "        \n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    \n",
    "    \n",
    "    # .clamp(0) is for the case when they DO NOT intersect. clamp(0) means set to 0 if it's less than 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    \n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    print(\"intersection coordinates: [\",x1.item(),y1.item(),x2.item(),y2.item(),\"]\")\n",
    "    print(\"intersection:\",intersection.item())\n",
    "    print(\"union:\",union.item())\n",
    "    \n",
    "    return (intersection / (union + 1e-6)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af66007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP(pred_boxses, true_boxes, iou_threshold=0.49, box_format=\"corners\", num_classes=20):\n",
    "    \n",
    "    # pred_boxes (list): [[train_idx, class_pred, prob_score, x1, y1, x2, y2], [...], ...]\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "        \n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        \n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "        \n",
    "        # ex)\n",
    "        # img 0 has 3 bboxes\n",
    "        # img 1 has 5 bboxes\n",
    "        # As a result: amount_bboxes ={0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        \n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "            \n",
    "        # amount_boxes = {0:torch.tesnor([0,0,0]), 1:torch.tensor([0,0,0,0,0])}\n",
    "        \n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "            \n",
    "            num_gts = len(ground_truth_img) # number of target bounding boxes in this image\n",
    "            \n",
    "            best_iou = 0\n",
    "            \n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = IoU(torch.tensor(detection[3:]), torch.tensor(gt[3:]), box_format=box_format)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "            \n",
    "            # Now we have a single bbox for particular class in a partiualr image\n",
    "            \n",
    "            if best_iou > iou_threshold:\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0: # This target bounding box has not yet been visited\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1 # Visited\n",
    "                else: # If already visited\n",
    "                    FP[detection_idx] = 1\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "                \n",
    "        # [1, 1, 0, 1, 0] -> [1,2,2,3,3]\n",
    "        TP_cumsum = torch.cumsum(TP, dim = 0) # cumsum -> prefix sum\n",
    "        FP_cumsum = torch.cumsum(FP, dim = 0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        \n",
    "        precisions = torch.cat((torch.tensor([1]), precisions)) # we need to have this for numerical integration\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        \n",
    "        average_precisions.append(torch.trapz(precision, recalls)) # trapz takes y, x\n",
    "        \n",
    "    return sum(average_precisions) / len(average_precisions) \n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2f60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
