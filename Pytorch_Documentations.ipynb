{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef23397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f917674",
   "metadata": {},
   "source": [
    "# Table Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd991d5",
   "metadata": {},
   "source": [
    "* [**1.** Tensor Basics](#tensor-basics)\n",
    "    * [**1.1** Tensor Initialization](#tensor-initialization)\n",
    "    * [**1.2** Data Type / Data Type Conversion](#data-type)\n",
    "    * [**1.3** Tensor Shape](#tensor-shape)\n",
    "    * [**1.4** Tensor Copy](#tensor-copy)\n",
    "    * [**1.5** Basic Operations](#basic-operations)\n",
    "        * Element-wise\n",
    "        * In-place\n",
    "    * [**1.6** Reshape](#reshape)\n",
    "    * [**1.7** Convert b/w Tensor and Numpy](#convert)\n",
    "* [**2.** Autograd](#autograd)\n",
    "    * [**2.1** Calculate Gradients](#calculate-gradients)\n",
    "    * [**2.2** Prevent Tracking Gradients](#prevent-tracking-grads)\n",
    "    * [**2.3** Reset(empty) Gradients](#reset-grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1a36a",
   "metadata": {},
   "source": [
    "# <a name=\"tensor-basics\">1. Tensor Basics</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44420e8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed89ae6",
   "metadata": {},
   "source": [
    "## <a name=\"tensor-initialization\">1.1 Tensor Initialization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ae060",
   "metadata": {},
   "source": [
    "<code>empty</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef3708c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "d1 = torch.empty(1)\n",
    "d2 = torch.empty(1,2)\n",
    "d3 = torch.empty(1,2,3)\n",
    "\n",
    "print(d1.shape)\n",
    "print(d2.shape)\n",
    "print(d3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ddb7e",
   "metadata": {},
   "source": [
    "<code>rand</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9198d8dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8599, 0.6840],\n",
      "        [0.7834, 0.5486]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e9c2b",
   "metadata": {},
   "source": [
    "<code>randn</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d83725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8464,  1.1656],\n",
      "        [ 1.5682,  0.2010]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96d031",
   "metadata": {},
   "source": [
    "<code>zeros</code> <code>ones</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0443bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2,2)\n",
    "y = torch.zeros(2,2)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd0276",
   "metadata": {},
   "source": [
    "<code>torch.tensor()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "31314692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000, 5.0000], dtype=torch.float16)\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.5, 0.1, 5], dtype=torch.float16)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645bfd99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ea2a5",
   "metadata": {},
   "source": [
    "## <a name=\"data-type\">1.2 Data Type</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bba4e1",
   "metadata": {},
   "source": [
    "### Check data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7ae6f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x type: torch.float32\n",
      "y type: torch.float16\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2)\n",
    "print(\"x type:\", x.dtype)\n",
    "\n",
    "y = torch.ones(2,2, dtype=torch.float16)\n",
    "print(\"y type:\", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f2cdb",
   "metadata": {},
   "source": [
    "### Convert data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5b385",
   "metadata": {},
   "source": [
    "<code>x.int()</code> <code>x.float()</code> <code>x.bool()</code> ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6f3fb15b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor:  tensor([0, 1, 2, 3])\n",
      "-------------------------------\n",
      "tensor.bool(): tensor([False,  True,  True,  True])\n",
      "tensor.short(): tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "tensor.long(): tensor([0, 1, 2, 3])\n",
      "tensor.half(): tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor.float(): tensor([0., 1., 2., 3.])\n",
      "tensor.double(): tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4) # dtype = int64\n",
    "print(\"tensor: \",x)\n",
    "print(\"-------------------------------\")\n",
    "print(\"tensor.bool():\",x.bool())\n",
    "print(\"tensor.short():\",x.short())\n",
    "print(\"tensor.long():\",x.long())\n",
    "print(\"tensor.half():\",x.half())\n",
    "print(\"tensor.float():\",x.float())\n",
    "print(\"tensor.double():\",x.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04c384",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dcd65",
   "metadata": {},
   "source": [
    "## <a name=\"tensor-shape\">1.3 Tensor Shape</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbed1b",
   "metadata": {},
   "source": [
    "<code>x.shape</code> <code>x.size()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cdcda628",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([3, 3])\n",
      "x shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 3, dtype=torch.double)\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"x shape: {x.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26446e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b145514",
   "metadata": {},
   "source": [
    "## <a name=\"tensor-copy\">1.4 Tensor Copy</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb47d48",
   "metadata": {},
   "source": [
    "<code>**x.clone().detach()**</code>\n",
    "\n",
    "* If you just assign <code>x = y</code>, you're passing the <code>reference</code> so you need to use <code>x = y.clone()</code>\n",
    "* <code>.detach()</code> is to separete from <code>computation graph</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "54698de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: tensor([[0.7119, 0.1766]])\n",
      "Original y: tensor([[0.7119, 0.1766]])\n",
      "------------------------------\n",
      "After Modifying x | .clone().detach()\n",
      "x: tensor([[1.7119, 1.1766]])\n",
      "y: tensor([[0.7119, 0.1766]])\n"
     ]
    }
   ],
   "source": [
    "# 1) x.clone().detach()\n",
    "x = torch.rand(1,2)\n",
    "y = x.clone().detach()\n",
    "print(f\"Original x: {x}\")\n",
    "print(f\"Original y: {y}\")\n",
    "\n",
    "# Modify \"x\"\n",
    "x.add_(1)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"After Modifying x | .clone().detach()\")\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "93e1a1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: tensor([[0.5889, 0.4031]])\n",
      "Original y: tensor([[0.5889, 0.4031]])\n",
      "------------------------------\n",
      "After Modifying x | x = y\n",
      "x: tensor([[1.5889, 1.4031]])\n",
      "y: tensor([[1.5889, 1.4031]])\n"
     ]
    }
   ],
   "source": [
    "# 2) Assignment Operator\n",
    "x = torch.rand(1,2)\n",
    "y = x\n",
    "print(f\"Original x: {x}\")\n",
    "print(f\"Original y: {y}\")\n",
    "\n",
    "# Modify \"x\"\n",
    "x.add_(1)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"After Modifying x | x = y\")\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434a01e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fdfa1",
   "metadata": {},
   "source": [
    "## <a name=\"basic-operations\">1.5 Basic Operations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f77ed",
   "metadata": {},
   "source": [
    "### 1) Element-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "12e237ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y: tensor([[1.3532, 0.2733],\n",
      "        [1.3901, 0.7752]])\n",
      "x - y: tensor([[-0.2460, -0.2085],\n",
      "        [-0.3234, -0.5713]])\n",
      "x * y: tensor([[0.4426, 0.0078],\n",
      "        [0.4569, 0.0686]])\n",
      "x / y: tensor([[0.6923, 0.1345],\n",
      "        [0.6225, 0.1514]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "\n",
    "add = x + y # torch.add(x,y)\n",
    "sub = x - y # torch.sub(x,y)\n",
    "mul = x * y # torch.mul(x,y)\n",
    "div = x / y # torch.div(x,y)\n",
    "\n",
    "print(f\"x + y: {add}\")\n",
    "print(f\"x - y: {sub}\")\n",
    "print(f\"x * y: {mul}\")\n",
    "print(f\"x / y: {div}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7a2f4",
   "metadata": {},
   "source": [
    "### 2) In-place operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "707fd85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1,2)\n",
    "y = torch.rand(1,2)\n",
    "x_temp = x.clone().detach()\n",
    "y_temp = y.clone().detach()\n",
    "\n",
    "# Normal Operations\n",
    "add = x + y # torch.add(x,y)\n",
    "sub = x - y # torch.sub(x,y)\n",
    "mul = x * y # torch.mul(x,y)\n",
    "div = x / y # torch.div(x,y)\n",
    "\n",
    "# Some Useful Functions\n",
    "def getClone(t):\n",
    "    return t.clone().detach()\n",
    "\n",
    "def isEqual(t1, t2):\n",
    "    comp = torch.eq(t1, t2) # torch.eq(t1, t2) <=> t1 == t2, element-wise comparison!\n",
    "    return torch.sum(comp) == torch.numel(t1)\n",
    "\n",
    "# In-place operations: (operator)_\n",
    "assert isEqual(x.add_(y),add) == True\n",
    "x = getClone(x_temp)\n",
    "\n",
    "assert isEqual(x.sub_(y),sub) == True\n",
    "x = getClone(x_temp)\n",
    "\n",
    "assert isEqual(x.mul_(y),mul) == True\n",
    "x = getClone(x_temp)\n",
    "\n",
    "assert isEqual(x.div_(y),div) == True\n",
    "x = getClone(x_temp)\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85710e59",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4cc3fa",
   "metadata": {},
   "source": [
    "## <a name=\"reshape\">1.6 Reshape</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737bea2",
   "metadata": {},
   "source": [
    "<code>x.view()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bdc595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "\n",
    "r1 = x.view(16)\n",
    "r2 = x.view(-1)\n",
    "r3 = x.view(-1, 8)\n",
    "print(r1.size())\n",
    "print(r2.size())\n",
    "print(r3.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f969a63",
   "metadata": {},
   "source": [
    "<code>x.reshape()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74e51b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "r1 = x.reshape(16)\n",
    "r2 = x.reshape(-1)\n",
    "r3 = x.reshape(-1, 8)\n",
    "print(r1.size())\n",
    "print(r2.size())\n",
    "print(r3.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219978f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f35fb",
   "metadata": {},
   "source": [
    "## <a name=\"convert\">1.7 Convert b/w Tensor and Numpy</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4de85",
   "metadata": {},
   "source": [
    "**NOTE**: Numpy is only available for <code>CPU</code>. So you <code>cannot</code> convert <code>GPU tensor</code> to <code>numpy</code>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a58ba",
   "metadata": {},
   "source": [
    "### Tensor -> Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee477a28",
   "metadata": {},
   "source": [
    "**WARNING**: If the tensor is on the <code>CPU</code>, then both will share the <code>same memory location<code/>!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e892630",
   "metadata": {},
   "source": [
    "<code>x.numpy()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f063b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([3., 3., 3., 3., 3.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.clone().detach().numpy() # Copy!\n",
    "\n",
    "a += 2\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c853c8",
   "metadata": {},
   "source": [
    "### Numpy -> Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b65c2",
   "metadata": {},
   "source": [
    "**WARNING**: If the tensor is on the <code>CPU</code>, then both will share the <code>same memory location<code/>!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f23c717",
   "metadata": {},
   "source": [
    "<code>torch.from_numpy()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f287e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [1. 1. 1. 1. 1.]\n",
      "[3. 3. 3. 3. 3.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "print(f\"a: {a}\")\n",
    "\n",
    "b = torch.from_numpy(a.copy())\n",
    "\n",
    "a += 2\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9ec63",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ad372",
   "metadata": {},
   "source": [
    "# <a name=\"autograd\">2. Autograd</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba988031",
   "metadata": {},
   "source": [
    "## <a name=\"calculate-gradients\">2.1 Calculate Gradients</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fff94a",
   "metadata": {},
   "source": [
    "* If we want to calculate the gradients of some function w.r.t <code>x</code>, we must specify do <code>requires_grad = True</code>\n",
    "* Whenever we do operations with the tensor <code>x</code>, pytorch will create <code>computational graph</code>.\n",
    "* **WARNING**: ONLY <code>Leaf variables</code> can be calculated for gradients\n",
    "* **WARNING**: Grad can be implicitly created only for scalar ouputs -> Otherwise, we have to put Jacobian vector to the parameter of .backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ee591",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66df3d4",
   "metadata": {},
   "source": [
    "#### Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6f610735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32., grad_fn=<MulBackward0>)\n",
      "tensor(80.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x * x * x * x * x # x^5\n",
    "print(y)\n",
    "\n",
    "y.backward() # dy/dx = d/dx(x^5) = 5x^4\n",
    "\n",
    "print(x.grad) # f'(x) = 5x^4, f'(2) = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6f489",
   "metadata": {},
   "source": [
    "#### Vector leaf variable (input layer in NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a524911",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([ 0.7533,  0.2624, -0.1996], requires_grad=True)\n",
      "y: tensor([2.7533, 2.2624, 1.8004], grad_fn=<AddBackward0>)\n",
      "z: tensor([15.1610, 10.2367,  6.4826], grad_fn=<MulBackward0>)\n",
      "output: tensor(10.6268, grad_fn=<MeanBackward0>)\n",
      "Grad of x: tensor([3.6710, 3.0165, 2.4005])\n"
     ]
    }
   ],
   "source": [
    "# x: leaf variable\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(\"x:\",x)\n",
    "\n",
    "y = x + 2\n",
    "print(\"y:\",y)\n",
    "\n",
    "z = y*y*2\n",
    "print(\"z:\",z)\n",
    "\n",
    "# scalar output\n",
    "out = z.mean()\n",
    "print(\"output:\",out)\n",
    "\n",
    "# Backprop\n",
    "out.backward() # dz/dx\n",
    "\n",
    "# Gradient\n",
    "print(\"Grad of x:\",x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f850fc",
   "metadata": {},
   "source": [
    "#### Only Scalar output! Otherwise, we should pass Jacobian vector to the parameter of .backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "680b4cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(4., grad_fn=<MulBackward0>)\n",
      "tensor(192.)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor(1.0, dtype=torch.float32, requires_grad=True)\n",
    "y = x * x # 4\n",
    "z = y * y * y # 64\n",
    "print(x)\n",
    "print(y)\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad) # YOU CANNOT DO THIS! Only \"LEAF\" variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36369dc2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3175165",
   "metadata": {},
   "source": [
    "## <a name=\"prevent-tracking-grads\">2.2 Prevent Tracking Gradients</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199fb9f",
   "metadata": {},
   "source": [
    "#### 1. <code>x.requires_grad = False</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3c0b22a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0114,  0.2078, -0.5200], requires_grad=True)\n",
      "tensor([-2.0114,  0.2078, -0.5200])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, dtype=torch.float32, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "# Set requires_grad = False\n",
    "x.requires_grad = False # or x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b209ed",
   "metadata": {},
   "source": [
    "#### 2. <code>y = x.detach()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "366eb0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1849,  0.9818,  1.8296], requires_grad=True)\n",
      "tensor([-0.1849,  0.9818,  1.8296])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, dtype=torch.float32, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c486fec7",
   "metadata": {},
   "source": [
    "#### 3. <code>with torch.no_grad():</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "600ecb14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4112, -0.8595, -0.6248], requires_grad=True)\n",
      "tensor([2.4112, 1.1405, 1.3752], grad_fn=<AddBackward0>)\n",
      "tensor([2.4112, 1.1405, 1.3752])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, dtype=torch.float32, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "# Torch is tracking grads of y\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Torch stops tracking grads of y\n",
    "    y = x + 2\n",
    "    print(y)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2eb731",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ebd47",
   "metadata": {},
   "source": [
    "## <a name=\"reset-grads\">2.3 [Important] Reset(empty) gradients </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b3134",
   "metadata": {},
   "source": [
    "### Grads are <code>accumulated</code>!!! Must <code>turn off</code> for new calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c922c8",
   "metadata": {},
   "source": [
    "### WRONG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fbd210d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, dtype=torch.float32, requires_grad = True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    out = (weights * 3).sum()\n",
    "    \n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f041c7",
   "metadata": {},
   "source": [
    "### CORRECT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e6ce2",
   "metadata": {},
   "source": [
    "<code>weights.grad.zero_()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cde0f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, dtype=torch.float32, requires_grad = True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    out = (weights * 3).sum()\n",
    "    \n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    # RESET\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec4a24",
   "metadata": {},
   "source": [
    "### Example - optimizer\n",
    "\n",
    "<code>optimizer.zero_grad()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "40f8e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([weights], lr=0.01)\n",
    "\n",
    "# Backprop\n",
    "optimizer.step()\n",
    "\n",
    "# Reset grad\n",
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f6b47",
   "metadata": {},
   "source": [
    "# <a name=\"backpropagation\">3. Backpropagation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c080c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "\n",
    "# forward pass\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y) ** 2\n",
    "print(\"loss:\",loss)\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "## update weights\n",
    "## next forward and backward...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e491e7",
   "metadata": {},
   "source": [
    "# <a name=\"gradient-descent\">4. Gradient Descent</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149b166",
   "metadata": {},
   "source": [
    "## <a name=\"manual\">4.1 Manually - Numpy </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0db84007",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 12: w = 2.000, loss = 0.00000005\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "Prediction before training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype = np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype = np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "\n",
    "# Gradients\n",
    "# MSE = 1/N * (w*X - Y) ** 2\n",
    "# dJ/dw = (1/N) * (2X) * (w*X - Y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "# Training\n",
    "lr = 0.01\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # Gradient Descents\n",
    "    w -= lr * dw\n",
    "    \n",
    "    print(f\"epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "    \n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa54bcf",
   "metadata": {},
   "source": [
    "## <a name=\"using-pytorch1\">4.2 Using Pytorch - Calculating Gradients</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "594feda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 0: w = 0.300, loss = 30.00000000\n",
      "epoch 10: w = 1.665, loss = 1.16278565\n",
      "epoch 20: w = 1.934, loss = 0.04506890\n",
      "epoch 30: w = 1.987, loss = 0.00174685\n",
      "epoch 40: w = 1.997, loss = 0.00006770\n",
      "epoch 50: w = 1.999, loss = 0.00000262\n",
      "epoch 60: w = 2.000, loss = 0.00000010\n",
      "epoch 70: w = 2.000, loss = 0.00000000\n",
      "epoch 80: w = 2.000, loss = 0.00000000\n",
      "epoch 90: w = 2.000, loss = 0.00000000\n",
      "Prediction before training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad = True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "\n",
    "# Gradients\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "# Training\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass\n",
    "    l.backward() # calculate dl/dw\n",
    "    \n",
    "    # Gradient Descents - update weights\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        \n",
    "    # zero gradients - prevent accumulate\n",
    "    w.grad.zero_()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "    \n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f4223",
   "metadata": {},
   "source": [
    "## <a name=\"using-pytorch3\">4.3 Training Pipeline - Full Pytorch</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87efac83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: w = 0.450, loss = 13.94171047\n",
      "epoch 10: w = 0.550, loss = 11.38415337\n",
      "epoch 20: w = 0.647, loss = 9.13728619\n",
      "epoch 30: w = 0.741, loss = 7.21859646\n",
      "epoch 40: w = 0.829, loss = 5.62261677\n",
      "epoch 50: w = 0.912, loss = 4.32642221\n",
      "epoch 60: w = 0.989, loss = 3.29690361\n",
      "epoch 70: w = 1.058, loss = 2.49665403\n",
      "epoch 80: w = 1.121, loss = 1.88787580\n",
      "epoch 90: w = 1.178, loss = 1.43474269\n",
      "epoch 100: w = 1.227, loss = 1.10481000\n",
      "epoch 110: w = 1.271, loss = 0.86979514\n",
      "epoch 120: w = 1.309, loss = 0.70589817\n",
      "epoch 130: w = 1.341, loss = 0.59378690\n",
      "epoch 140: w = 1.369, loss = 0.51829404\n",
      "epoch 150: w = 1.393, loss = 0.46792859\n",
      "epoch 160: w = 1.412, loss = 0.43428683\n",
      "epoch 170: w = 1.429, loss = 0.41143200\n",
      "epoch 180: w = 1.443, loss = 0.39531589\n",
      "epoch 190: w = 1.455, loss = 0.38327187\n",
      "epoch 200: w = 1.466, loss = 0.37360388\n",
      "epoch 210: w = 1.475, loss = 0.36527169\n",
      "epoch 220: w = 1.483, loss = 0.35765946\n",
      "epoch 230: w = 1.490, loss = 0.35041875\n",
      "epoch 240: w = 1.496, loss = 0.34335861\n",
      "epoch 250: w = 1.503, loss = 0.33638170\n",
      "epoch 260: w = 1.508, loss = 0.32944199\n",
      "epoch 270: w = 1.514, loss = 0.32252043\n",
      "epoch 280: w = 1.520, loss = 0.31561211\n",
      "epoch 290: w = 1.525, loss = 0.30871826\n",
      "epoch 300: w = 1.531, loss = 0.30184427\n",
      "epoch 310: w = 1.536, loss = 0.29499531\n",
      "epoch 320: w = 1.541, loss = 0.28817770\n",
      "epoch 330: w = 1.547, loss = 0.28139716\n",
      "epoch 340: w = 1.552, loss = 0.27465981\n",
      "epoch 350: w = 1.558, loss = 0.26797086\n",
      "epoch 360: w = 1.563, loss = 0.26133549\n",
      "epoch 370: w = 1.569, loss = 0.25475860\n",
      "epoch 380: w = 1.575, loss = 0.24824481\n",
      "epoch 390: w = 1.580, loss = 0.24179830\n",
      "epoch 400: w = 1.586, loss = 0.23542327\n",
      "epoch 410: w = 1.591, loss = 0.22912319\n",
      "epoch 420: w = 1.597, loss = 0.22290187\n",
      "epoch 430: w = 1.602, loss = 0.21676221\n",
      "epoch 440: w = 1.608, loss = 0.21070734\n",
      "epoch 450: w = 1.614, loss = 0.20474021\n",
      "epoch 460: w = 1.619, loss = 0.19886288\n",
      "epoch 470: w = 1.625, loss = 0.19307792\n",
      "epoch 480: w = 1.630, loss = 0.18738742\n",
      "epoch 490: w = 1.636, loss = 0.18179306\n",
      "epoch 500: w = 1.642, loss = 0.17629650\n",
      "epoch 510: w = 1.647, loss = 0.17089939\n",
      "epoch 520: w = 1.653, loss = 0.16560282\n",
      "epoch 530: w = 1.658, loss = 0.16040803\n",
      "epoch 540: w = 1.664, loss = 0.15531574\n",
      "epoch 550: w = 1.669, loss = 0.15032685\n",
      "epoch 560: w = 1.675, loss = 0.14544176\n",
      "epoch 570: w = 1.680, loss = 0.14066112\n",
      "epoch 580: w = 1.685, loss = 0.13598500\n",
      "epoch 590: w = 1.691, loss = 0.13141355\n",
      "epoch 600: w = 1.696, loss = 0.12694693\n",
      "epoch 610: w = 1.701, loss = 0.12258478\n",
      "epoch 620: w = 1.706, loss = 0.11832707\n",
      "epoch 630: w = 1.712, loss = 0.11417322\n",
      "epoch 640: w = 1.717, loss = 0.11012278\n",
      "epoch 650: w = 1.722, loss = 0.10617518\n",
      "epoch 660: w = 1.727, loss = 0.10232972\n",
      "epoch 670: w = 1.732, loss = 0.09858569\n",
      "epoch 680: w = 1.737, loss = 0.09494206\n",
      "epoch 690: w = 1.742, loss = 0.09139784\n",
      "epoch 700: w = 1.747, loss = 0.08795218\n",
      "epoch 710: w = 1.752, loss = 0.08460376\n",
      "epoch 720: w = 1.757, loss = 0.08135138\n",
      "epoch 730: w = 1.761, loss = 0.07819396\n",
      "epoch 740: w = 1.766, loss = 0.07513005\n",
      "epoch 750: w = 1.771, loss = 0.07215826\n",
      "epoch 760: w = 1.775, loss = 0.06927716\n",
      "epoch 770: w = 1.780, loss = 0.06648547\n",
      "epoch 780: w = 1.785, loss = 0.06378146\n",
      "epoch 790: w = 1.789, loss = 0.06116365\n",
      "epoch 800: w = 1.793, loss = 0.05863055\n",
      "epoch 810: w = 1.798, loss = 0.05618041\n",
      "epoch 820: w = 1.802, loss = 0.05381174\n",
      "epoch 830: w = 1.806, loss = 0.05152274\n",
      "epoch 840: w = 1.811, loss = 0.04931187\n",
      "epoch 850: w = 1.815, loss = 0.04717733\n",
      "epoch 860: w = 1.819, loss = 0.04511746\n",
      "epoch 870: w = 1.823, loss = 0.04313055\n",
      "epoch 880: w = 1.827, loss = 0.04121495\n",
      "epoch 890: w = 1.831, loss = 0.03936882\n",
      "epoch 900: w = 1.835, loss = 0.03759053\n",
      "epoch 910: w = 1.838, loss = 0.03587832\n",
      "epoch 920: w = 1.842, loss = 0.03423052\n",
      "epoch 930: w = 1.846, loss = 0.03264537\n",
      "epoch 940: w = 1.850, loss = 0.03112124\n",
      "epoch 950: w = 1.853, loss = 0.02965631\n",
      "epoch 960: w = 1.857, loss = 0.02824910\n",
      "epoch 970: w = 1.860, loss = 0.02689777\n",
      "epoch 980: w = 1.864, loss = 0.02560077\n",
      "epoch 990: w = 1.867, loss = 0.02435648\n",
      "Prediction before training: f(5) = 9.720\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1) Design model (input, output size, forward pass)\n",
    "2) Construct lossand optimizer\n",
    "3) Training loop\n",
    "    * forward pass: compute prediction\n",
    "    * backward pass: gradients\n",
    "    * gradient descent(update weights)\n",
    "    '''\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w * x\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32) # num_samples x data\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32) # 4 x 1\n",
    "X_test = torch.tensor([[5]], dtype=torch.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# model\n",
    "input_size = X.shape[1] # X.shape = (# of samples, # of features)\n",
    "output_size = X.shape[1]\n",
    "\n",
    "'''[METHOD 1]'''\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, out_features)\n",
    "    def forward(self, x):\n",
    "        return self.linear1(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "'''\n",
    "[METHOD 2]\n",
    "model = nn.Linear(in_features=input_size, out_features=output_size, bias = True)\n",
    "\n",
    "[METHOD 3]\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, output_size, bias = True)\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "# Loss Function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass\n",
    "    l.backward() # calculate dl/dw\n",
    "    \n",
    "    # Gradient Descents - update weights\n",
    "    optimizer.step()\n",
    "        \n",
    "    # zero gradients - prevent accumulate\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f\"epoch {epoch}: w = {w[0][0].item():.3f}, loss = {l:.8f}\")\n",
    "    \n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53875f9f",
   "metadata": {},
   "source": [
    "# <a name=\"linear-regression\">5. Linear Regression</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "229eaf4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0| Loss: 5616.82568359375\n",
      "epoch 1000| Loss: 4377.82958984375\n",
      "epoch 2000| Loss: 3402.62158203125\n",
      "epoch 3000| Loss: 2592.921630859375\n",
      "epoch 4000| Loss: 1928.9398193359375\n",
      "epoch 5000| Loss: 1398.1383056640625\n",
      "epoch 6000| Loss: 990.4639892578125\n",
      "epoch 7000| Loss: 695.9095458984375\n",
      "epoch 8000| Loss: 502.3505859375\n",
      "epoch 9000| Loss: 393.37835693359375\n",
      "epoch 10000| Loss: 346.4350280761719\n",
      "epoch 11000| Loss: 333.9825744628906\n",
      "epoch 12000| Loss: 332.60198974609375\n",
      "epoch 13000| Loss: 332.5676574707031\n",
      "epoch 14000| Loss: 332.56756591796875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "<Training Pipeline>\n",
    "[1] Prepare Data\n",
    "[2] Model\n",
    "[3] Loss/Optimizer\n",
    "[4] Training Loop\n",
    "'''\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 15000\n",
    "\n",
    "# [1] Prepare Data\n",
    "X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
    "\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "\n",
    "# [1] Model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, output_size)\n",
    ")\n",
    "\n",
    "# [2] Loss/Optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# [3] Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forwardprop\n",
    "    Y_pred = model(X)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = loss_fn(Y, Y_pred)\n",
    "    \n",
    "    # Backwardprop\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient Descent\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Zero-out gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"epoch {epoch}| Loss: {loss}\")\n",
    "    \n",
    "\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, Y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc434ea",
   "metadata": {},
   "source": [
    "# <a name=\"logistic-regression\">6. Logistic Regression</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1596fdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (569, 30)\n",
      "epoch 0| Loss: 0.476\n",
      "epoch 1000| Loss: 0.025\n",
      "epoch 2000| Loss: 0.018\n",
      "epoch 3000| Loss: 0.013\n",
      "epoch 4000| Loss: 0.009\n",
      "epoch 5000| Loss: 0.005\n",
      "epoch 6000| Loss: 0.003\n",
      "epoch 7000| Loss: 0.002\n",
      "epoch 8000| Loss: 0.001\n",
      "epoch 9000| Loss: 0.001\n",
      "epoch 10000| Loss: 0.001\n",
      "accuracy: 0.9649122953414917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10000\n",
    "\n",
    "# [1] Prepare Data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, Y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(\"X shape:\",X.shape)\n",
    "\n",
    "### Train/Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
    "\n",
    "### Scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "### Convert numpy to torch\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "Y_train = torch.from_numpy(Y_train.astype(np.float32))\n",
    "Y_test = torch.from_numpy(Y_test.astype(np.float32))\n",
    "\n",
    "### Fit data shape\n",
    "Y_train = Y_train.unsqueeze(1)\n",
    "Y_test = Y_test.unsqueeze(1)\n",
    "\n",
    "# [2] Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear1(x))\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# [3] Loss/Optimizer\n",
    "loss_fn = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# [4] Training Loop\n",
    "for epoch in range(num_epochs+1):\n",
    "    Y_pred = model(X_train)\n",
    "    loss = loss_fn(Y_pred, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"epoch {epoch}| Loss: {loss.item():.3f}\")\n",
    "\n",
    "# [5] Accuracy\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred_classes = (y_pred > 0.5) # or y_predicted.round()\n",
    "    accuracy = (y_pred_classes == Y_test).sum() / float(Y_test.shape[0])\n",
    "    print(\"accuracy:\", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f0a9d",
   "metadata": {},
   "source": [
    "# <a name=\"dataset-dataloader\">7. Dataset and Dataloader</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "34809efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db53aba",
   "metadata": {},
   "source": [
    "## <a name=\"dataset\">7.1 Dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "97e4b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # dataloading\n",
    "        xy = np.loadtxt('./test_data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:])\n",
    "        self.y = torch.from_numpy(xy[:, [0]]) # (n_samples, 1)\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # dataset[index]\n",
    "        return self.x[index], self.y[index] # return a tuple\n",
    "        \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = WineDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecacc54",
   "metadata": {},
   "source": [
    "### Quick Look at custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "afe68f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 14)\n",
      "Dataset Length: 178\n",
      "Each element of dataset is: <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('./test_data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "print(xy.shape) # Both X and Y\n",
    "dataset = WineDataset()\n",
    "print(f\"Dataset Length: {len(dataset)}\")\n",
    "print(\"Each element of dataset is:\",type(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705defd",
   "metadata": {},
   "source": [
    "## <a name=\"dataloader\">7.2 Dataloader</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b70bd41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st batch X: torch.Size([4, 13])\n",
      "1st batch Y: torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset = WineDataset()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "\n",
    "# iterator\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = data_iter.next()\n",
    "\n",
    "print(\"1st batch X:\",data[0].shape)\n",
    "print(\"1st batch Y:\",data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64771df8",
   "metadata": {},
   "source": [
    "### Training Loop using dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "020f8c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X in each batch: torch.Size([4, 13])\n",
      "Y in each batch: torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.floor(total_samples / 4)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (x, y) in enumerate(dataloader):\n",
    "        # forwardprop \n",
    "        # loss\n",
    "        # backprop\n",
    "        # gradient descent\n",
    "        if(epoch == 0 and batch_idx == 0):\n",
    "            print(\"X in each batch:\",x.shape)\n",
    "            print(\"Y in each batch:\",y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f82ac",
   "metadata": {},
   "source": [
    "## <a name=\"torchvision-datasets\">7.3 torchvision datasets - mnist, coco, cifar, ...</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296746f0",
   "metadata": {},
   "source": [
    "### torchvision datasets\n",
    "\n",
    "##### MNIST, coco, cifar, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6b4cdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = datasets.MNIST(root='datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae321d72",
   "metadata": {},
   "source": [
    "# <a name=\"dataset-transforms\">8. Dataset Transforms</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "eae19ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        # dataloading\n",
    "        xy = np.loadtxt('./test_data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        \n",
    "        # Note that we didn't convert numpy to tensor here unlike above\\\n",
    "        self.x = x[:, 1:].numpy()\n",
    "        self.y = y[:, 0:1].numpy()\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # dataset[index]\n",
    "        sample = self.x[index], self.y[index] # return a tuple\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = WineDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb143c95",
   "metadata": {},
   "source": [
    "### ToTensor: numpy -> tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ae5c8",
   "metadata": {},
   "source": [
    "#### Without ToTensor Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f1d873af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "first_data = dataset[0]\n",
    "\n",
    "features, labels = first_data\n",
    "\n",
    "print(type(features))\n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff66bd",
   "metadata": {},
   "source": [
    "#### With ToTensor Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "fe86d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        # sample - callable object\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "85b177da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WineDataset(transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "543ef9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "first_data = dataset[0]\n",
    "\n",
    "features, labels = first_data\n",
    "\n",
    "print(type(features))\n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791ebab",
   "metadata": {},
   "source": [
    "### Compose Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "f5ddb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MulTransform:\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "be0840f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    MulTransform(2)\n",
    "])\n",
    "\n",
    "dataset = WineDataset(transform=my_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "06f00590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "first_data = dataset[0]\n",
    "\n",
    "features, labels = first_data\n",
    "\n",
    "print(type(features))\n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3543751",
   "metadata": {},
   "source": [
    "# <a name=\"softmas-crossentropy\">9. Softmax and Cross-Entropy</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf986f1",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbd8462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "output = softmax(x)\n",
    "print('softmax numpy:', output)\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "output = torch.softmax(x, dim = 0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ad869",
   "metadata": {},
   "source": [
    "### Cross-Entropy\n",
    "\n",
    "#### D(Y_hat, Y) = -(1/N) * sum(Y, log(Y_hat))\n",
    "### [WARNING] \n",
    "* nn.CrossEntropyLoss already applied softmax - we must not implement softmax on our own!\n",
    "* Y has class labels, not ONE-HOT!\n",
    "* Y_pred has raw scores (logits), no Softmax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "250713c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 numpy: 0.3567\n",
      "Loss2 numpy: 2.3026\n",
      "--------------------------------------------------\n",
      "Loss1 torch: 0.3018\n",
      "Loss2 torch: 1.6242\n",
      "tensor([2, 0, 1])\n",
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(y, y_hat):\n",
    "    loss = -np.sum(y * np.log(y_hat))\n",
    "    return loss\n",
    "\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "y_hat_good = np.array([0.7, 0.2, 0.1])\n",
    "y_hat_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(Y, y_hat_good)\n",
    "l2 = cross_entropy(Y, y_hat_bad)\n",
    "\n",
    "print(f\"Loss1 numpy: {l1:.4f}\")\n",
    "print(f\"Loss2 numpy: {l2:.4f}\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "###### PYTORCH ########\n",
    "\n",
    "# 3 samples \n",
    "Y = torch.tensor([2, 0, 1]) # only put correct class labels, not one-hot encoded / (nsamples x nclasses)\n",
    "\n",
    "# n_samples x n_classes = 3 x 3\n",
    "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1],[2.0, 1.0, 0.1],[0.1, 3.0, 0.1]]) # raw value -> no softmax!\n",
    "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1],[0.1, 1.0, 2.1],[0.1, 3.0, 0.1]])\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(f\"Loss1 torch: {l1.item():.4f}\")\n",
    "print(f\"Loss2 torch: {l2.item():.4f}\")\n",
    "\n",
    "_, pred1 = torch.max(Y_pred_good, dim=1)\n",
    "_, pred2 = torch.max(Y_pred_bad, dim=1)\n",
    "\n",
    "print(pred1)\n",
    "print(pred2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b9a66",
   "metadata": {},
   "source": [
    "# <a name=\"feedforward-net\">10. Feed Forward Net</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2d32e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 784\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "be6e8970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 28, 28])\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e7a80a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9f43a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5d7bca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f84d3acc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0| loss:2.2924070358276367\n",
      "epoch0| loss:0.31495508551597595\n",
      "epoch1| loss:0.2921135425567627\n",
      "epoch1| loss:0.29330411553382874\n",
      "epoch2| loss:0.228903129696846\n",
      "epoch2| loss:0.20731891691684723\n",
      "epoch3| loss:0.22773289680480957\n",
      "epoch3| loss:0.16507260501384735\n",
      "epoch4| loss:0.15580104291439056\n",
      "epoch4| loss:0.15195779502391815\n",
      "epoch5| loss:0.17612873017787933\n",
      "epoch5| loss:0.17884905636310577\n",
      "epoch6| loss:0.1344349980354309\n",
      "epoch6| loss:0.12804560363292694\n",
      "epoch7| loss:0.13447771966457367\n",
      "epoch7| loss:0.12602375447750092\n",
      "epoch8| loss:0.09830492734909058\n",
      "epoch8| loss:0.09955698251724243\n",
      "epoch9| loss:0.11981098353862762\n",
      "epoch9| loss:0.09619618207216263\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x = x.reshape(-1, 28*28).to(device)\n",
    "        # y = shape(64), NO one-hot encoding! just leave as it is\n",
    "        \n",
    "        #forward\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        #loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        #backward\n",
    "        loss.backward()\n",
    "        \n",
    "        #gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        #zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"epoch{epoch}| loss:{loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f91a3d6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 97.12\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x = x.reshape(-1, 28*28).to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        _, predictions = torch.max(y_pred, dim=1)\n",
    "        n_samples += y.shape[0]\n",
    "        n_correct += (predictions == y).sum().item()\n",
    "        \n",
    "    accuracy = 100.0 * n_correct / n_samples\n",
    "    print(f\"accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd712c",
   "metadata": {},
   "source": [
    "# <a name=\"cnn\">11. CNN</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a93d70e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "my_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=my_transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=my_transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c78daa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "example = iter(train_loader)\n",
    "samples, labels = example.next()\n",
    "print(samples.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5547a8",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "75dcfe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.reshape(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # No Activation or Softmax cuz Cross-Entropy already includes them\n",
    "        return x\n",
    "        \n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87626b4",
   "metadata": {},
   "source": [
    "### Loss / Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "180315b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491cc9cd",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "02825684",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0| Loss:2.3194258213043213\n",
      "epoch1| Loss:2.2163162231445312\n",
      "epoch2| Loss:1.9535205364227295\n",
      "epoch3| Loss:2.016279458999634\n",
      "epoch4| Loss:1.6039097309112549\n",
      "epoch5| Loss:1.461698293685913\n",
      "epoch6| Loss:1.566805124282837\n",
      "epoch7| Loss:1.2426338195800781\n",
      "epoch8| Loss:1.7180901765823364\n",
      "epoch9| Loss:1.3615368604660034\n",
      "epoch10| Loss:1.448979377746582\n",
      "epoch11| Loss:1.4639530181884766\n",
      "epoch12| Loss:1.2979403734207153\n",
      "epoch13| Loss:1.0745251178741455\n",
      "epoch14| Loss:1.0791819095611572\n",
      "epoch15| Loss:1.0769681930541992\n",
      "epoch16| Loss:1.0116138458251953\n",
      "epoch17| Loss:1.2534679174423218\n",
      "epoch18| Loss:1.0635095834732056\n",
      "epoch19| Loss:1.1331738233566284\n",
      "epoch20| Loss:1.129142165184021\n",
      "epoch21| Loss:1.069625735282898\n",
      "epoch22| Loss:1.3173127174377441\n",
      "epoch23| Loss:1.0502465963363647\n",
      "epoch24| Loss:0.854570746421814\n",
      "epoch25| Loss:1.045387864112854\n",
      "epoch26| Loss:1.1977635622024536\n",
      "epoch27| Loss:0.8352165818214417\n",
      "epoch28| Loss:1.040642261505127\n",
      "epoch29| Loss:1.031670093536377\n",
      "epoch30| Loss:0.8199904561042786\n",
      "epoch31| Loss:0.9812594056129456\n",
      "epoch32| Loss:0.7883741855621338\n",
      "epoch33| Loss:0.8085212707519531\n",
      "epoch34| Loss:0.764972984790802\n",
      "epoch35| Loss:1.0784698724746704\n",
      "epoch36| Loss:0.5936248898506165\n",
      "epoch37| Loss:0.7702489495277405\n",
      "epoch38| Loss:0.89811110496521\n",
      "epoch39| Loss:0.8258076310157776\n",
      "epoch40| Loss:0.8706024289131165\n",
      "epoch41| Loss:1.0009843111038208\n",
      "epoch42| Loss:0.718536376953125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cp/hj2ty6s546gg413qfzt3915h0000gn/T/ipykernel_71404/3977956893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(f\"epoch{epoch}| Loss:{loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17abfbfa",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3eea35dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.88221153846154\n",
      "Accuracy of plane: 68.00401203610832\n",
      "Accuracy of car: 76.02808425275828\n",
      "Accuracy of bird: 51.80360721442886\n",
      "Accuracy of cat: 41.18236472945892\n",
      "Accuracy of deer: 65.36536536536536\n",
      "Accuracy of dog: 54.154154154154156\n",
      "Accuracy of frog: 69.06906906906907\n",
      "Accuracy of horse: 69.9\n",
      "Accuracy of ship: 80.74222668004012\n",
      "Accuracy of truck: 62.6\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # max returns\n",
    "        _, predicted = torch.max(y_pred, dim=1)\n",
    "        n_samples += y.shape[0]\n",
    "        n_correct += (predicted == y).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = y[i]\n",
    "            predicted_label = predicted[i]\n",
    "            \n",
    "            if label == predicted_label:\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "        \n",
    "    accuracy = 100.0 * n_correct / n_samples\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f\"Accuracy of {classes[i]}: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85139086",
   "metadata": {},
   "source": [
    "# <a name=\"transfer-learning\">12. Transfer Learning</a>\n",
    "\n",
    "* Look up furthermore: ImageFolder, lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d40cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8a20b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/jasonlee/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7595de7e92a64d9690969ed535e3b6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadd23ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb5b922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.in_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb74c9ae",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c07c8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the layers before up to fc\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d39ccd",
   "metadata": {},
   "source": [
    "### Loss / Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b9624db",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21c901",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "107463c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) # every 7 epochs, lr is multiplied by 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ce20f",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e436b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train_model(model, loss_fn, optimizer, scheduler, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a93121",
   "metadata": {},
   "source": [
    "# <a name=\"tensorboard\">13. Tensorboard</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b21434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48abbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca90428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
